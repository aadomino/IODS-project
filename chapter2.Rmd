# Regression and model validation
*The objective of this week was learning, performing and interpreting the results of regression analysis. This part includes code, intrepretations and explanations of the results obtained with blood, sweat and tears.*

## 1. Reading and exploring the data

The data used in this part comes from an international survey of approaches to learning - see more [on this page.](http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt)).

You can find the pre-processed data here: [GitHub data repository.](https://github.com/adomino/IODS-project/data)

The dataset, *learning2014*, consists of 166 observations and 7 variables - these are the dimensions of the data. You can see it here:

```{r}
learning2014 <- read.table("C:/Users/P8Z77-V/Documents/learning2014.csv", header = TRUE, sep = "\t")
dim(learning2014)
```

It is also possible to observe the data structure of the data frame:

```{r}
str(learning2014)
```

The variables occurring in the data are *gender, age, attitude, deep, stra, surf*, and *points*. 

These variables are used to categorise the answers of students from the survey. The questions pertained to the students' assessment of their deep-, strategic-, and surface learning, and the data was also collected on their age, gender, and attitude towards statistics.

The dataset does not contain the students who received 0 points from the final exam. These results have been filtered out: 

```{r}
learning2014 <- dplyr::filter(learning2014, points > 0)
```

## 2. Taking a more detailed look at the data

#### Summary of the variables

The summary of the data includes a lot of information on all the variables. There are minimum, maximum, median and mean values of each category, and the first and the third quintiles of the data.

```{r}
summary(learning2014)
```

An overview of this dataset produces a few interesting observations. We can clearly see that there are more female respondents than male ones, and the age variable represents typical age distribution of university students. Most of them are in their twenties, with a few outliers with the ages from 17 to 55. The attitude variable shows that the survey participants approach statistics with a slightly more positive than negative attitude. Deep learning is favoured more than strategic or surface learning (deep, stra and surf variables). The final exam results range from 7 to 33.

#### A graphical overview

With a graphical plot we can actually visualise the variables and the relationships between them.

```{r}
# Access the GGally and ggplot2 libraries.
library(GGally)
library(ggplot2)

# Read a plot matrix with ggpairs() into a variable p0. Draw.
p0 <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p0

```

It is a complex plot. One thing that stands out is that there is some positive correlation between exam points received by the students and their attitude towards statistics, but the correlation between exam points and other variables is generally weak. In general, the correlations between variables in our dataset are not strong.

This strong correlation makes sense: if a student has a positive outlook on statistics (as we all do), they are more likely to learn and obtain good results on the final test. It is possible to visualise this particular relation in more detail. Here, it is done by plotting the variables *attitude* and *points* as a scatterplot. The colour codes gender. Regression lines are also included:

```{r}
# Access the ggplot2 library.
library(ggplot2)

# Draw the plot (p1) with our data. Define the mapping. Define the visualization type (dots) and smoothing. Add the plot title.
p1 <- ggplot(learning2014, aes(x = attitude, y = points, col = gender)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Students' attitude towards statistics vs final exam points")
p1
```

## 3. Choosing and fitting a regression model

In this part we will choose and fit a suitable regression model, which will explain the data in more detail - we want to find out which factors influence the amount of exam points received.
*Points* is the target (dependent) variable. The previous section shows that *attitude*, *stra* and *surf* correlate most strongly with *points*. They will be our explanatory variables in this model, whose summary is printed out below:

```{r}
# Fit a regression model (m0) with multiple explanatory variables: attitude, stra, surf. Print a summary of the model.
m0 <- lm(points ~ attitude + stra + surf, data = learning2014)
summary(m0)
```

The results show the variables used in the model.

Residuals are assumed to be normally distributed with zero mean and constant variance. The median is indeed close to zero and the residuals seem to follow normal distribution.

Coefficients show estimated influence of the explanatory variables on the target variable - the logistic probability of the outcome for a change by one unit in explanatory variable. In other words, here, if attitude increases by 1, the logistic odds of better test points increase by 0.33952. The more excited the students are about the subject, the better chances they have to pass the final with flying colours. 

The summary also shows standard error, t- and p-values and indicates the significance values. The effect of attitude on the dependent variable (exam points) is statistically significant, while stra and surf are not (p value over .5) If an explanatory variable in the model does not have a statistically significant relationship with the target variable, we remove the variable from the model and fit the model again without it. In this summary, the residuals' median has decreased and attitude is highly statistically significant.

```{r}
# Create a regression model m1 with only attitude. Print a summary of the model.
m1 <- lm(points ~ attitude, data = learning2014)
summary(m1)
```

## 4. Interpreting the results

The second summary indicates that the estimated effect of students' attitude on exam results is 3.5255. Again, this means that for each unit increase in attitude, the exam results are also expected to increase.

The multiple R-squared value evaluates how much of the changes (variance) of the target variable is explained by the model. The rest of the variance is explained by some other factors that are not included. It could be understood as a goodness of fit measure.

The multiple R-squared is higher in the first model, even though the explanatory variables were shown to be statistically not significant and were subsequently dropped. The value increases when any variables are added to the model, irrespective of their significance.

## 5. Creating diagnostic plots


```{r}
# Diagnostic plots using the plot() function. Choose the plots 1, 2 and 5.
par(mfrow = c(2,2))
plot(m1, which = c(1,2,5))
```









